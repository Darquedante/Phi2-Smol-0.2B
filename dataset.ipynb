{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import ujson\n",
    "import numpy as np\n",
    "from rich import progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 处理预训练阶段数据\n",
    "## 1.1 处理Wiki数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_wiki_file = './data/wiki.simple.txt'\n",
    "\n",
    "liness = []\n",
    "with open(origin_wiki_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并词条和内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items, content = [], []\n",
    "key_word, kw_line_idx = '', 0\n",
    "content_start = False  # 词条内容开始标记\n",
    "\n",
    "bos_token, eos_token = '[BOS]', '[EOS]' \n",
    "for i, line in enumerate(lines):\n",
    "    \n",
    "    line_strip = line.strip()\n",
    "\n",
    "    # 词条以冒号`：`结尾\n",
    "    if len(line_strip) > 0 and line_strip[-1] in (':', '：'):\n",
    "        key_word = ''.join(line_strip[: -1])\n",
    "        kw_line_idx = i \n",
    "        continue\n",
    "    \n",
    "    # 词条key_word在下一行，则合并上个词条并保存\n",
    "    if i == kw_line_idx + 1 and key_word in line_strip or i == len(lines) - 1:\n",
    "        txt = ''.join(content)\n",
    "\n",
    "        if len(txt) > 0:\n",
    "            items.append(f\"{txt}{eos_token}\")\n",
    "            \n",
    "        content = []\n",
    "        content.append(f\"{key_word}：\")\n",
    "    \n",
    "    content.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Wiki数据合并为长度固定的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_txt_cropus_to_chunk_data(texts: list[str], batch_size: int=512 ** 2, max_len: int=320, window_size: int = 2) -> list[str]:\n",
    "    \n",
    "    buffer, buffer_len = [], 0\n",
    "    chunk_data = []\n",
    "\n",
    "    for i, line in enumerate(texts):\n",
    "        buffer_len += len(line)\n",
    "        buffer.append(line)\n",
    "\n",
    "        if buffer_len >= batch_size or i == len(texts) - 1:\n",
    "            buffer_txt = ''.join(buffer)\n",
    "            \n",
    "            # - window_size为滑动窗口，这样每个窗口都包含有window_size个上文\n",
    "            for i in range(0, len(buffer_txt), max_len - window_size):\n",
    "\n",
    "                chunk_data.append(''.join(buffer_txt[i: i + max_len]))\n",
    "            \n",
    "            buffer, buffer_len = [], 0\n",
    "    \n",
    "    return chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2045355\n"
     ]
    }
   ],
   "source": [
    "chunk_data = split_txt_cropus_to_chunk_data(items)\n",
    "print(len(chunk_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data[0: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pa.Table.from_arrays([chunk_data], names=['text'])\n",
    "# compression='GZIP'\n",
    "pq.write_table(table=tb, where='./data/wiki_chunk_320_2.2M.parquet', row_group_size=50000, data_page_size=50000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_samples = np.random.choice(chunk_data, size=100_0000).tolist()\n",
    "wiki_samples = chunk_data[0: 10_0000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_samples[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2 处理百度百科数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import ujson\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_txt_cropus_to_chunk_data(texts: list[str], batch_size: int=512 ** 2, max_len: int=320, window_size: int = 2) -> list[str]:\n",
    "    \n",
    "    buffer, buffer_len = [], 0\n",
    "    chunk_data = []\n",
    "\n",
    "    for i, line in enumerate(texts):\n",
    "        buffer_len += len(line)\n",
    "        buffer.append(line)\n",
    "\n",
    "        if buffer_len >= batch_size or i == len(texts) - 1:\n",
    "            buffer_txt = ''.join(buffer)\n",
    "            \n",
    "            # - window_size为滑动窗口，这样每个窗口都包含有window_size个上文\n",
    "            for i in range(0, len(buffer_txt), max_len - window_size):\n",
    "\n",
    "                chunk_data.append(''.join(buffer_txt[i: i + max_len]))\n",
    "            \n",
    "            buffer, buffer_len = [], 0\n",
    "    \n",
    "    return chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_baike_563w_file = './data/563w_baidubaike.json'\n",
    "baike_items = []\n",
    "eos_token = '[EOS]' \n",
    "max_len = 320\n",
    "batch_size, batch_cnt = 200_0000, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bd_baike_563w_file, 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    line = normalize('NFKC', line)\n",
    "    item = ujson.loads(line)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bd_baike_563w_file, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    def process_none(s: str) -> str:\n",
    "        if s: return s\n",
    "        return ''\n",
    "    \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "\n",
    "        item = ujson.loads(line)\n",
    "        cur_txt, cur_len = [], 0\n",
    "\n",
    "        if not item['title']: continue\n",
    "\n",
    "        temp_txt = f\"{item['title']}：{process_none(item['summary'])}\"\n",
    "        \n",
    "        cur_len += len(temp_txt)\n",
    "        cur_txt.append(temp_txt)\n",
    "\n",
    "        for section in item['sections']:\n",
    "\n",
    "            # 太长的截断不要了\n",
    "            if cur_len > max_len:\n",
    "                break\n",
    "            \n",
    "            title = f\"{section['title']}：\" if section['title'] else \"\"\n",
    "            temp_txt = f\"{title}{process_none(section['content'])}\"\n",
    "            \n",
    "            cur_len += len(temp_txt)\n",
    "            cur_txt.append(temp_txt)\n",
    "        \n",
    "        # normalize 处理\\u3000 \\xa0，全角转半角\n",
    "        temp_txt =  normalize('NFKC', ''.join(cur_txt))\n",
    "\n",
    "        if len(temp_txt) > max_len:\n",
    "            # 从 max_len 开始找第一个句号，叹号\n",
    "            n, i = len(temp_txt), max_len\n",
    "            while i < n and temp_txt[i] not in ('。', '！'):\n",
    "                i += 1\n",
    "            temp_txt = ''.join(temp_txt[0: i + 1])\n",
    "\n",
    "        # 添加 eos token\n",
    "        temp_txt = f\"{temp_txt}{eos_token}\"\n",
    "        \n",
    "        baike_items.append( temp_txt )\n",
    "\n",
    "        if len(baike_items) % batch_size == 0:\n",
    "\n",
    "            chunk_data = split_txt_cropus_to_chunk_data(baike_items)\n",
    "            tb = pa.Table.from_arrays([chunk_data], names=['text'])\n",
    "\n",
    "            file_name = f'./data/baike_chunk_320_5.6M_{batch_cnt}.parquet'\n",
    "            pq.write_table(table=tb, where=file_name, row_group_size=50000, )\n",
    "\n",
    "            print(f\"save to {file_name}\")\n",
    "\n",
    "            batch_cnt += 1\n",
    "            baike_items = []\n",
    "\n",
    "    if len(baike_items) > 0:\n",
    "        chunk_data = split_txt_cropus_to_chunk_data(baike_items)\n",
    "        tb = pa.Table.from_arrays([chunk_data], names=['text'])\n",
    "\n",
    "        file_name = f'./data/baike_chunk_320_5.6M_{batch_cnt}.parquet'\n",
    "        pq.write_table(table=tb, where=file_name, row_group_size=50000, )\n",
    "\n",
    "        print(f\"save to {file_name}\")\n",
    "\n",
    "        batch_cnt += 1\n",
    "        baike_items = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\n",
    "    f'./data/baike_chunk_320_5.6M_{batch_cnt}.parquet' for batch_cnt in range(3)\n",
    "]\n",
    "\n",
    "line_cnt = 0 \n",
    "for file in file_list:\n",
    "    pf = pq.read_table(file)\n",
    "    line_cnt += pf.num_rows\n",
    "\n",
    "print(f\"bake all lines: {line_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data[20: 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 处理bell指令数据\n",
    "尝试在预训练阶段加入prompt指令数据，就是尝试在预训练解决加加入部分Sft数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "eval_data = []\n",
    "eval_size = 1_0000\n",
    "max_len = 400\n",
    "root = 'D:/GitHub/ChatLM-mini-Chinese/data/raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root + '/bell_open_source/train_3.5M_CN.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = ujson.loads(line)\n",
    "\n",
    "        if len(item['conversations']) != 2: continue\n",
    "\n",
    "        conversation = item['conversations']\n",
    "        txt = ''\n",
    "        if conversation[0]['from'] =='human':\n",
    "            txt = f\"{conversation[0]['value']}\\n{conversation[1]['value']}\"\n",
    "        else:\n",
    "            txt = f\"{conversation[1]['value']}\\n{conversation[0]['value']}\"\n",
    "        \n",
    "         # 收集测试数据\n",
    "        if len(txt) >= max_len and len(txt) < max_len + 8 and len(eval_data) < eval_size and np.random.rand() <= 0.12:\n",
    "            eval_data.append(txt)\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if len(txt) >= max_len: continue\n",
    "        train_data.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5429 1084177\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_data), len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in [root + '/bell_open_source/train_2M_CN.json',  root + '/bell_open_source/Belle_open_source_1M.json']:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = ujson.loads(line)\n",
    "\n",
    "            if item['input'].strip() != '':\n",
    "                txt = f\"{item['instruction']}\\n{item['input']}\\n{item['output']}\"\n",
    "            else:\n",
    "                txt = f\"{item['instruction']}\\n{item['output']}\"\n",
    "\n",
    "            # 收集测试数据\n",
    "            if len(txt) >= max_len and len(txt) < max_len + 8 and len(eval_data) < eval_size and np.random.rand() > 0.75:\n",
    "                eval_data.append(txt)\n",
    "                continue\n",
    "            \n",
    "            if len(txt) == 0 or len(txt) >= max_len: continue\n",
    "            train_data.append(\n",
    "                    txt\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 3150704\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_data), len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pa.Table.from_arrays([train_data], names=['text'])\n",
    "# compression='GZIP'\n",
    "pq.write_table(table=tb, where=f'./data/bell_pretrain_{max_len}_3M.parquet', row_group_size=20480, data_page_size=20480, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pa.Table.from_arrays([eval_data], names=['text'])\n",
    "# compression='GZIP'\n",
    "pq.write_table(table=tb, where=f'./data/pretrain_eval_{max_len}_1w.parquet', row_group_size=20480, data_page_size=20480, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 处理sft阶段数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('./data/sft_0.8M_CN.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = ujson.loads(line)\n",
    "\n",
    "        txt = f\"{item['instruction']}{item['output']}\"\n",
    "        \n",
    "        if len(txt) == 0 or len(txt) >= 320: continue\n",
    "        lines.append(\n",
    "                item\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726475\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pa.Table.from_pylist(lines)\n",
    "# compression='GZIP'\n",
    "pq.write_table(table=tb, where='./data/sft_train_data.parquet', row_group_size=20480, data_page_size=20480, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计 token数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet as pq\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('./model_save/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符数量\n",
    "files = [\n",
    "    './data/baike_chunk_320_5.6M_0.parquet', \n",
    "    './data/baike_chunk_320_5.6M_1.parquet', \n",
    "    './data/baike_chunk_320_5.6M_2.parquet', \n",
    "    './data/bell_pretrain_400_3M.parquet',\n",
    "    # './data/pretrain_eval_400_1w.parquet',\n",
    "]\n",
    "\n",
    "total_char = 0\n",
    "for file in files: \n",
    "    pf = pq.read_table(file)\n",
    "    for row in pf['text']:\n",
    "        total_char += len(row.as_py())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token = 0\n",
    "buffer = []\n",
    "for file in files: \n",
    "    pf = pq.read_table(file)\n",
    "    n = pf.num_rows\n",
    "    for i, row in tqdm(enumerate(pf['text']), total=n):\n",
    "        buffer.append(row.as_py())\n",
    "\n",
    "        if len(buffer) >= 10000 or i == n - 1:\n",
    "            input_ids = tokenizer(buffer, return_attention_mask=False)['input_ids']\n",
    "            \n",
    "            total_token += sum([len(item) for item in input_ids])\n",
    "            buffer = []\n",
    "\n",
    "if len(buffer) > 0:\n",
    "    input_ids = tokenizer(buffer, return_attention_mask=False)['input_ids']\n",
    "    \n",
    "    total_token += sum([len(item) for item in input_ids])\n",
    "    buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

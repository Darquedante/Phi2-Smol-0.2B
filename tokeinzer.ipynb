{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('./model_save/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train the tokenizer (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Digits, Metaspace, ByteLevel\n",
    "from tokenizers.normalizers import NFKC \n",
    "from rich import progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the source of the tokenizer training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropus_file =  './data/wiki.simple.txt'\n",
    "tokenizer_save_path = './model_save/hf_bpe_tokenizer.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to train the tokenizer\n",
    "The `get_training_corpus` function concatenates multiple short texts into sentences longer than `chunk_len=2048`, returning `buffer_size=1000` of such long sentences each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_my_huggingface_wiki_tokenizer(max_train_line: int=None, token_type: str='char') -> None:\n",
    "    '''\n",
    "    Train tokenizer with huggingface, at least 32GB of memory needed, about half an hour to run.\n",
    "    '''\n",
    "\n",
    "    # if not exists(tokenizer_save_path): mkdir(tokenizer_save_path)\n",
    "\n",
    "    def get_training_corpus(buffer_size: int=1000, chunk_len: int=2048) -> list:\n",
    "        '''\n",
    "        A text chunk of size 2048\n",
    "        '''\n",
    "        line_cnt = 0\n",
    "        buffer = []\n",
    "        with open(cropus_file, 'r', encoding='utf-8') as f_read:\n",
    "            cur_chunk_txt, txt_len = [], 0\n",
    "            for line in f_read:\n",
    "\n",
    "                cur_chunk_txt.append(line)\n",
    "                txt_len += len(line)\n",
    "                line_cnt += 1\n",
    "\n",
    "                if txt_len >= chunk_len:\n",
    "                    buffer.append(\n",
    "                        ''.join(cur_chunk_txt)\n",
    "                    )\n",
    "                    cur_chunk_txt, txt_len = [], 0\n",
    "                \n",
    "                if len(buffer) >= buffer_size:\n",
    "                    yield buffer\n",
    "                    buffer = []\n",
    "\n",
    "                if isinstance(max_train_line, int) and line_cnt > max_train_line: break\n",
    "                \n",
    "            # yield last\n",
    "            if len(buffer) > 0: yield buffer        \n",
    "\n",
    "    special_tokens = [\"[PAD]\",\"[EOS]\",\"[SEP]\",\"[BOS]\", \"[CLS]\", \"[MASK]\", \"[UNK]\"]\n",
    "    \n",
    "    if token_type ==' char':\n",
    "        model = BPE(unk_token=\"[UNK]\")\n",
    "        tokenizer = Tokenizer(model)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Use compatible equivalent decomposition and recombination to process utf encoding, e.g., converting full-width A to half-width A\n",
    "        tokenizer.normalizer = tokenizers.normalizers.Sequence([NFKC()])\n",
    "\n",
    "        # Pre-tokenization for punctuation, digits, and Metaspace (otherwise, decoded text will not have spaces)\n",
    "        tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Sequence(\n",
    "            [Punctuation(), Digits(individual_digits=True), Metaspace()]\n",
    "        )\n",
    "\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizer.decoder = decoders.Metaspace()\n",
    "    elif token_type ==' byte':\n",
    "        # Byte BPE does not need unk_token\n",
    "        model = BPE() \n",
    "        tokenizer = Tokenizer(model)\n",
    "        tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=True)\n",
    "\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False, use_regex=True)\n",
    "        tokenizer.post_processor = tokenizers.processors.ByteLevel(trim_offsets=False)\n",
    "    else:\n",
    "        raise Exception('Token type must be `char` or `byte`')\n",
    "\n",
    "    trainer = BpeTrainer(vocab_size=40960, min_frequency=100, show_progress=True, special_tokens=special_tokens)\n",
    "    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "    # add \\t \\n \n",
    "    if '\\t' not in tokenizer.get_vocab():\n",
    "        tokenizer.add_tokens(['\\t'])\n",
    "    if '\\n' not in tokenizer.get_vocab():\n",
    "        tokenizer.add_tokens(['\\n'])\n",
    "\n",
    "    tokenizer.save(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Start training the tokenizer\n",
    "At least `32GB` of memory needed for 100 million characters (actually, `32GB` is still not quite enough, frequent swapping may occur), CPU `13600k` takes about an hour for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_my_huggingface_wiki_tokenizer(token_type='byte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convert the trained tokenizer to PreTrainedTokenizerFast and save\n",
    "Conversion is for ease of use as `AutoTokenizer` in other `huggingface` components.\n",
    "\n",
    "During conversion, manually specify `pad_token`, `eos_token`, etc., as it doesn't automatically identify which characters in the original tokenizer are these special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = Tokenizer.from_file(tokenizer_save_path)\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=slow_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    bos_token='[BOS]',\n",
    "    eos_token='[EOS]',                  \n",
    ")\n",
    "tokenizer.save_pretrained('./model_save/fast_tokenizer/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
